{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "matplotlib.rcParams['savefig.dpi'] = 1.5 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "import numpy as np\n",
    "import math, sys, os, glob\n",
    "\n",
    "import george\n",
    "import george.kernels as kernels\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "Msol = 1.98855*10.0**30.0\n",
    "yr = 365.25 * 86400.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## These are just some matplotlib formatting options\n",
    "\n",
    "def figsize(scale):\n",
    "    fig_width_pt = 469.755                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = fig_width*golden_mean              # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "params = {'backend': 'pdf',\n",
    "        'axes.labelsize': 10,\n",
    "        'lines.markersize': 4,\n",
    "        'font.size': 10,\n",
    "        'xtick.major.size':6,\n",
    "        'xtick.minor.size':3,  \n",
    "        'ytick.major.size':6,\n",
    "        'ytick.minor.size':3, \n",
    "        'xtick.major.width':0.5,\n",
    "        'ytick.major.width':0.5,\n",
    "        'xtick.minor.width':0.5,\n",
    "        'ytick.minor.width':0.5,\n",
    "        'lines.markeredgewidth':1,\n",
    "        'axes.linewidth':1.2,\n",
    "        'legend.fontsize': 7,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'savefig.dpi':200,\n",
    "        'path.simplify':True,\n",
    "        'font.family': 'serif',\n",
    "        'font.serif':'Times',\n",
    "        'text.latex.preamble': [r'\\usepackage{amsmath}'],\n",
    "        'text.usetex':True,\n",
    "        'axes.color_cycle': ['b', 'lime', 'r', 'purple', 'g', 'c', 'm', 'orange', 'darkblue', \\\n",
    "                                'darkcyan', 'y','orangered','chartreuse','brown','deeppink','lightgreen', 'k'],\n",
    "        'figure.figsize': (3.39,2.1)}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read in the population synthesis spectra. These spectra cover a range of stellar density values and initial binary eccentricities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filnames = glob.glob('./data/*.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = []\n",
    "bb = []\n",
    "for ii in range(len(filnames)):\n",
    "    aa.append(filnames[ii].split('_')[-3].split('Rho')[1])\n",
    "    bb.append(filnames[ii].split('_')[-2].split('Ecc')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = np.unique(np.array(aa))\n",
    "bb = np.unique(np.array(bb))\n",
    "\n",
    "aa = aa[np.argsort(aa.astype(float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "ct = 0\n",
    "for stars in aa:\n",
    "    for ecc in bb:\n",
    "        data.append(np.loadtxt('./data/Spectra_Rho{0}_Ecc{1}_HundredYears.gz'.format(str(stars),str(ecc))))\n",
    "        print ct, stars, ecc\n",
    "        ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "\n",
    "aa = aa.astype(float) / 100.0 # log10 of stellar density\n",
    "bb = bb.astype(float) / 1000.0 # eccentricity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First of all, let's just take a look at the spectra from the extreme corners of astrophysical environmental conditions.\n",
    "\n",
    "## This is Fig. 1 of Taylor, Simon, Sampson (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = [0,13,168,181] \n",
    "labs = ['\\\\textbf{(a)}', '\\\\textbf{(b)}', '\\\\textbf{(c)}', '\\\\textbf{(d)}']\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2,ncols=2,sharex=True,sharey=True)\n",
    "\n",
    "ctx=0\n",
    "cty=0\n",
    "for kk,ind in enumerate(inds):\n",
    "\n",
    "    newfreqs = np.arange(1.0,90.0) / (30.0*365.25*86400.0)\n",
    "    newwidth = newfreqs[1]-newfreqs[0]\n",
    "    newstrain = np.zeros((len(newfreqs),100))\n",
    "\n",
    "    finefreqs = data[ind,0,:]\n",
    "    finestrain = data[ind,1:,:]\n",
    "    finewidth = finefreqs[1] - finefreqs[0]\n",
    "    \n",
    "    for ii in range(len(newfreqs)):\n",
    "        for jj in range(data.shape[1]-1):\n",
    "            \n",
    "            mask = np.logical_and(finefreqs >= (newfreqs[ii] - newwidth/2.), finefreqs <= (newfreqs[ii] + newwidth/2.))\n",
    "            newstrain[ii,jj] = np.sum(finestrain.T[mask,jj]) * finewidth / newwidth \n",
    "\n",
    "    # compute mean and std from original binning\n",
    "    mean = np.mean(np.log10(np.sqrt(data[ind,:,:])),axis=0)\n",
    "    err = np.std(np.log10(np.sqrt(data[ind,1:,:])),axis=0)\n",
    "\n",
    "    # compute mean and std from new binning\n",
    "    newmean = np.mean(np.log10(np.sqrt(newstrain)),axis=1)\n",
    "    newerr = np.std(np.log10(np.sqrt(newstrain)),axis=1)\n",
    "    \n",
    "    if ind == 0:\n",
    "        ctx,cty = 0, 0\n",
    "    elif ind == 13:\n",
    "        ctx,cty = 0, 1\n",
    "    elif ind == 168:\n",
    "        ctx,cty = 1, 0\n",
    "    elif ind == 181:\n",
    "        ctx,cty = 1, 1\n",
    "\n",
    "    ax[ctx,cty].plot(newfreqs, 10.0**newmean, color='r', alpha=0.5, label=labs[kk])\n",
    "    ax[ctx,cty].plot(newfreqs, 6e-16 * (newfreqs * 365.25*86400.0)**(-2./3.), \n",
    "             color='b', ls='dashed', alpha=0.5)\n",
    "    \n",
    "    for ii in range(100):\n",
    "        ax[ctx,cty].plot(data[ind,0,:], np.sqrt(data[ind,1+ii,:]), color='k', lw=1.2, alpha=0.01)\n",
    "    \n",
    "    ax[ctx,cty].set_xscale('log')\n",
    "    ax[ctx,cty].set_yscale('log')\n",
    "    ax[ctx,cty].set_xlim(1e-9,1.0/(365.25*86400.0))\n",
    "    ax[ctx,cty].set_ylim(1e-16,2e-14)\n",
    "    leg = ax[ctx,cty].legend(loc='upper right',frameon=False,handlelength=0, handletextpad=0)\n",
    "    for item in leg.legendHandles:\n",
    "        item.set_visible(False)\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.text(2e-10,1.5e-17,'GW Frequency [Hz]')\n",
    "plt.text(8e-12,1e-12,'Characteristic strain, $h_c(f)$',rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have the characteristic strain spectra starting at 1/(100-yrs). Let's re-bin the spectra for a 30-yr baseline instead of 100-yrs. \n",
    "\n",
    "## Also, let's just look at the characteristic strain at one frequency, f_gw = 1/(30-yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_ind = 0\n",
    "\n",
    "newfreqs = np.arange(1.0,100.0) / (30.0*365.25*86400.0)\n",
    "newwidth = newfreqs[1]-newfreqs[0]\n",
    "newstrain = np.zeros((data.shape[0],data.shape[1]-1))\n",
    "\n",
    "finefreqs = data[0,0,:] # same for all parameter inputs\n",
    "finestrain = data[:,1:,:]\n",
    "finewidth = finefreqs[1] - finefreqs[0]\n",
    "\n",
    "for ii in range(data.shape[0]):\n",
    "    for jj in range(data.shape[1]-1):\n",
    "        \n",
    "        mask = np.logical_and(finefreqs >= (newfreqs[freq_ind] - newwidth/2.), \n",
    "                              finefreqs <= (newfreqs[freq_ind] + newwidth/2.))\n",
    "        \n",
    "        newstrain[ii,jj] = np.sum(finestrain[ii,jj,mask]) * finewidth / newwidth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mean strain spectrum and std (from Poissonian realization variance) from new binning. This becomes our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newmean = np.mean(np.log10(np.sqrt(newstrain)/1e-15),axis=1)\n",
    "newerr = np.std(np.log10(np.sqrt(newstrain)/1e-15),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort data in 2-d space of density and eccentricity into one long vector\n",
    "\n",
    "xobs = np.zeros((len(aa)*len(bb),2))\n",
    "    \n",
    "A, B = np.meshgrid(aa,bb)\n",
    "\n",
    "xobs[:,0] = A.flatten(order='F') # iterates over all eccentricities then all stellar densities\n",
    "xobs[:,1] = B.flatten(order='F')\n",
    "\n",
    "yobs = newmean\n",
    "yerr = newerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have the data. So let's set up the Gaussian Process (GP), using the George GP regression library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use a Squared-Exponential kernel\n",
    "\n",
    "k = 1.0 * kernels.ExpSquaredKernel([0.5,0.5],ndim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the GP\n",
    "\n",
    "gp = george.GP(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-compute the factorization of the matrix.\n",
    "\n",
    "gp.compute(xobs, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the log likelihood of the data, given the GP model.\n",
    "\n",
    "print(gp.lnlikelihood(yobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK. Let's try predicting new values for the strain spectrum between the training points, i.e. at new values of stellar-density and eccentricity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define prediction points\n",
    "\n",
    "t = np.zeros((10000,2))\n",
    "    \n",
    "xx = np.linspace(1.0,4.0,100) # log10(\\rho / Msol.pc^{-3}) in U[1.0,4.0] \n",
    "yy = np.linspace(0.0,0.95,100) # e_0 in U[1.0,4.0] \n",
    "\n",
    "X, Y = np.meshgrid(xx,yy)\n",
    "\n",
    "t[:,0] = X.flatten(order='F')\n",
    "t[:,1] = Y.flatten(order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the characteristic strain across astrophysical parameter space, and give a prediction uncertainity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu, cov = gp.predict(yobs, t)\n",
    "std = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above is fine, but really we should train our GP kernel parameters on the simulation data. So, let's optimize the kernel parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a GP class containing the kernel parameter priors and a log-likelihood\n",
    "\n",
    "class gaussproc(object):\n",
    "    \n",
    "    def __init__(self, x, y, yerr=None):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.yerr = yerr\n",
    "        \n",
    "        self.pmax = np.array([20.0, 20.0, 20.0])\n",
    "        self.pmin = np.array([-20.0, -20.0, -20.0])\n",
    "        self.emcee_flatchain = None\n",
    "        self.emcee_flatlnprob = None\n",
    "        self.emcee_kernel_map = None\n",
    "    \n",
    "    def lnprior(self, p):\n",
    "    \n",
    "        logp = 0.\n",
    "    \n",
    "        if np.all(p <= self.pmax) and np.all(p >= self.pmin):\n",
    "            logp = np.sum(np.log(1/(self.pmax-self.pmin)))\n",
    "        else:\n",
    "            logp = -np.inf\n",
    "\n",
    "        return logp\n",
    "\n",
    "    def lnlike(self, p):\n",
    "\n",
    "        # Update the kernel and compute the lnlikelihood.\n",
    "        a, tau = np.exp(p[0]), np.exp(p[1:3])\n",
    "        \n",
    "        lnlike = 0.0\n",
    "        try:\n",
    "            gp = george.GP(a * kernels.ExpSquaredKernel(tau,ndim=2))\n",
    "            #gp = george.GP(a * kernels.Matern32Kernel(tau))\n",
    "            gp.compute(self.x , self.yerr)\n",
    "            \n",
    "            lnlike = gp.lnlikelihood(self.y, quiet=True)\n",
    "        except np.linalg.LinAlgError:\n",
    "            lnlike = -np.inf\n",
    "        \n",
    "        return lnlike\n",
    "    \n",
    "    def lnprob(self, p):\n",
    "        \n",
    "        return self.lnprior(p) + self.lnlike(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate a GP again \n",
    "\n",
    "gp_george = gaussproc(xobs,yobs,yerr)\n",
    "    \n",
    "k = 1.0 * kernels.ExpSquaredKernel([2.0,2.0],ndim=2)\n",
    "num_kpars = len(k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use emcee (http://dan.iel.fm/emcee/current/) to sample the \n",
    "# posterior distribution of the kernel parameters to find MAP value\n",
    "\n",
    "nwalkers, ndim = 36, num_kpars\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, gp_george.lnprob)\n",
    "\n",
    "# Initialize the walkers.\n",
    "p0 = [np.log([1.,1.,1.]) + 1e-4 * np.random.randn(ndim)\n",
    "      for i in range(nwalkers)]\n",
    "\n",
    "print \"Running burn-in\"\n",
    "p0, lnp, _ = sampler.run_mcmc(p0, 500)\n",
    "sampler.reset()\n",
    "\n",
    "print \"Running second burn-in\"\n",
    "p = p0[np.argmax(lnp)]\n",
    "p0 = [p + 1e-8 * np.random.randn(ndim) for i in xrange(nwalkers)]\n",
    "p0, _, _ = sampler.run_mcmc(p0, 500)\n",
    "sampler.reset()\n",
    "\n",
    "print \"Running production\"\n",
    "p0, _, _ = sampler.run_mcmc(p0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distribution of these kernel parameters.\n",
    "# The second and third parameters are the natural-log of the kernel length-scales\n",
    "# in the stellar-density and eccentricity parameters, respectively.\n",
    "\n",
    "fig = corner.corner(sampler.flatchain,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's grab the MAP values\n",
    "\n",
    "mapparams = np.exp(sampler.flatchain[np.argmax(sampler.flatlnprobability)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a GP kernel with the MAP values found from the sampling.\n",
    "\n",
    "k = mapparams[0] * kernels.ExpSquaredKernel(mapparams[1:],ndim=2)\n",
    "\n",
    "# Instanciate the GP.\n",
    "gp = george.GP(k)\n",
    "\n",
    "# Pre-compute the factorization of the matrix.\n",
    "gp.compute(xobs, yerr)\n",
    "\n",
    "# Compute the log likelihood.\n",
    "print(gp.lnlikelihood(yobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the strain at new astrophysical parameter locations between the training.\n",
    "# This time though, we use the fully trained GP.\n",
    "\n",
    "mu, cov = gp.predict(yobs, t)\n",
    "std = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great, so let's now plot the grid of training points, the new predictions across parameter space from the trained GP, and the prediction uncertainties.\n",
    "\n",
    "## This is Fig. 2 from Taylor, Simon, Sampson (2016). For full details, see the relevant figure caption in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "\n",
    "im1 = axes[0].imshow(mu.reshape((100,100),order='F')-15.0,extent=(xx.min(),xx.max(),yy.max(),yy.min()),\n",
    "               cmap=matplotlib.cm.cubehelix, aspect=3.5)#, vmin=0.0579768135068413, vmax=1.0)\n",
    "im2 = axes[1].imshow(std.reshape((100,100),order='F'),extent=(xx.min(),xx.max(),yy.max(),yy.min()),\n",
    "               cmap=matplotlib.cm.cubehelix, aspect=3.5)#, vmin=0.0579768135068413, vmax=1.0)\n",
    "\n",
    "for ii in range(xobs.shape[0]):\n",
    "    axes[0].scatter(x=xobs[ii,0],y=xobs[ii,1],\n",
    "                    alpha=0.4,color='r',s=5,marker='.',clip_on=False)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.minorticks_on() \n",
    "    ax.set_xlim(xx.min(),xx.max())\n",
    "    ax.set_ylim(yy.max(),yy.min())\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "plt.setp(axes[1].get_yticklabels(), visible=False)\n",
    "\n",
    "axes[0].set_ylabel(r'$e_0$')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.2,right=0.9)\n",
    "cbar_ax = fig.add_axes([0.125, 0.85, 0.35, 0.04])\n",
    "fig.colorbar(im1, cax=cbar_ax, orientation='horizontal')\n",
    "cbar_ax.set_xticklabels(cbar_ax.get_xticklabels(),rotation=45,size=7)\n",
    "cbar_ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.55, 0.85, 0.35, 0.04])\n",
    "fig.colorbar(im2, cax=cbar_ax, orientation='horizontal')\n",
    "cbar_ax.set_xticklabels(cbar_ax.get_xticklabels(),rotation=45,size=7)\n",
    "cbar_ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "plt.text(-0.5,-22.0,r'$\\log_{10}(\\rho / M_\\odot\\mathrm{pc}^{-3})$')\n",
    "plt.text(-1.1,5.0,r'$\\log_{10}[h_c(f=1\\;\\mathrm{nHz})]$',fontsize=8.5)\n",
    "plt.text(0.06,5.0,r'$\\Delta \\log_{10}[h_c(f=1\\;\\mathrm{nHz})]$',fontsize=8.5)\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse   \n",
    "ellipse = Ellipse(xy=(2.5, 0.475), width=10.02123372/1., height=0.82797949/1., \n",
    "                  edgecolor='r', fc='None', lw=1.5, alpha=0.8)\n",
    "axes[1].add_patch(ellipse)\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig('gp4ptas_starsecc_trained.pdf',bbox_inches='tight',dpi=400,rasterized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# We now have the full formalism in place. Let's repreat the above for all frequencies in the GW spectrum, not just the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tobs = 30.0 # years\n",
    "newfreqs = np.arange(1.0,61.0) / (Tobs*365.25*86400.0)\n",
    "\n",
    "newwidth = newfreqs[1]-newfreqs[0]\n",
    "newstrain = np.zeros((data.shape[0],data.shape[1]-1))\n",
    "\n",
    "finefreqs = data[0,0,:] # same for all parameter inputs\n",
    "finestrain = data[:,1:,:]\n",
    "finewidth = finefreqs[1] - finefreqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packs the 2-d data into one long 1-d vector\n",
    "\n",
    "xobs = np.zeros((len(aa)*len(bb),2))\n",
    "A, B = np.meshgrid(aa,bb)\n",
    "xobs[:,0] = A.flatten(order='F') # iterates over all eccentricities then all stellar densities\n",
    "xobs[:,1] = B.flatten(order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Re-bin the spectra from a baseline of 100-yrs to 30-yrs.\n",
    "## This time for all frequencies, 1/(30-yrs) up to 60/(30-yrs).\n",
    "\n",
    "## Note that we have divided through by 4.5e-31 which is the \n",
    "## squared-strain at 1/yr.\n",
    "\n",
    "yobs = []\n",
    "yerr = []\n",
    "for freq_ind in range(len(newfreqs)):\n",
    "    for ii in range(data.shape[0]):\n",
    "        for jj in range(data.shape[1]-1):\n",
    "            mask = np.logical_and(finefreqs >= (newfreqs[freq_ind] - newwidth/2.), \n",
    "                                  finefreqs <= (newfreqs[freq_ind] + newwidth/2.))\n",
    "            newstrain[ii,jj] = np.sum(finestrain[ii,jj,mask]) * finewidth / newwidth \n",
    "\n",
    "    # compute mean and std from new binning\n",
    "    newmean = np.log10(np.mean(newstrain/4.5e-31,axis=1)) \n",
    "    \n",
    "    # cut out bad data, where some population realizations \n",
    "    # give zero strain at certain frequencies\n",
    "    logstrain = np.log10(newstrain/4.5e-31)\n",
    "    logstrain[logstrain==-np.inf] = 0.0\n",
    "    \n",
    "    newerr =  np.std(logstrain,axis=1) \n",
    "    \n",
    "    yobs.append(newmean)\n",
    "    yerr.append(newerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate a list of GP kernels and models [one for each frequency]\n",
    "\n",
    "gp_george = []\n",
    "k = []\n",
    "\n",
    "for freq_ind in range(len(newfreqs)):\n",
    "    \n",
    "    gp_george.append(gaussproc(xobs,yobs[freq_ind],yerr[freq_ind]))\n",
    "    k.append( 1.0 * kernels.ExpSquaredKernel([2.0,2.0],ndim=2) )\n",
    "    num_kpars = len(k[freq_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample the posterior distribution of the kernel parameters \n",
    "# to find MAP value for each frequency. \n",
    "\n",
    "# THIS WILL TAKE A WHILE... (~ 1 hr)\n",
    "\n",
    "sampler = [0.0]*len(newfreqs)\n",
    "for freq_ind in range(len(newfreqs)):\n",
    "    \n",
    "    # Set up the sampler.\n",
    "    nwalkers, ndim = 36, num_kpars\n",
    "    sampler[freq_ind] = emcee.EnsembleSampler(nwalkers, ndim, gp_george[freq_ind].lnprob)\n",
    "\n",
    "    # Initialize the walkers.\n",
    "    p0 = [np.log([1.,1.,1.]) + 1e-4 * np.random.randn(ndim)\n",
    "          for i in range(nwalkers)]\n",
    "\n",
    "    print freq_ind, \"Running burn-in\"\n",
    "    p0, lnp, _ = sampler[freq_ind].run_mcmc(p0, 500)\n",
    "    sampler[freq_ind].reset()\n",
    "\n",
    "    print freq_ind, \"Running second burn-in\"\n",
    "    p = p0[np.argmax(lnp)]\n",
    "    p0 = [p + 1e-8 * np.random.randn(ndim) for i in xrange(nwalkers)]\n",
    "    p0, _, _ = sampler[freq_ind].run_mcmc(p0, 500)\n",
    "    sampler[freq_ind].reset()\n",
    "\n",
    "    print freq_ind, \"Running production\", '\\n'\n",
    "    p0, _, _ = sampler[freq_ind].run_mcmc(p0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's take a look at the posterior distribution of the \n",
    "# kernel parameters at a frequency [ind] of our choice.\n",
    "\n",
    "ind = 59\n",
    "\n",
    "fig = corner.corner(sampler[ind].flatchain,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Populate the GP class with the details of the kernel \n",
    "## MAP values for each frequency.\n",
    "\n",
    "for ii in range(len(newfreqs)):\n",
    "    \n",
    "    gp_george[ii].chain = None \n",
    "    gp_george[ii].lnprob = None \n",
    "    \n",
    "    gp_george[ii].kernel_map = sampler[ii].flatchain[np.argmax(sampler[ii].flatlnprobability)] \n",
    "    print ii, gp_george[ii].kernel_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store GP object for each frequency as a pickle. \n",
    "\n",
    "# This pickle object is now ready to be used in NX01 to constrain astrophysical parameters using real PTA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(gp_george, open( \"starsecc_gp_13x14nodes_30yr.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final plot. Provide a stellar-density and eccentricity, and let's look at the strain-spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain_predict = []\n",
    "t = [np.array([3.35,0.65])] # log10(\\rho) = 3.35, e_0 = 0.65\n",
    "\n",
    "for ii in range(len(newfreqs)):\n",
    "    \n",
    "    # Let's grab the MAP values\n",
    "    mapparams = np.exp(sampler[ii].flatchain[np.argmax(sampler[ii].flatlnprobability)])\n",
    "    \n",
    "    # Set up a GP kernel with the MAP values found from the sampling.\n",
    "    k = mapparams[0] * kernels.ExpSquaredKernel(mapparams[1:],ndim=2)\n",
    "    \n",
    "    # Instanciate the GP.\n",
    "    gp = george.GP(k)\n",
    "    \n",
    "    # Pre-compute the factorization of the matrix.\n",
    "    gp.compute(xobs, yerr[ii])\n",
    "    \n",
    "    mu, cov = gp.predict(yobs[ii], t)\n",
    "    std = np.sqrt(np.diag(cov))\n",
    "    strain_predict.append(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.loglog(newfreqs[:20], np.sqrt(4.5e-31 * 10.0**np.array(strain_predict[:20])),\n",
    "          alpha=0.5)\n",
    "plt.xlabel(r'GW Frequency [Hz]')\n",
    "plt.ylabel(r'Characteristic strain, $h_c(f)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
